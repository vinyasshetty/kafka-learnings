## Consumer

* Maximum number of consumers you can have in a consumer groups should be equal to the number of partitions of a given topic.
* In other words, one partition cannot send data to more then one consumer within a consumer group.But we can have multiple partition sending data to one consumer within a consumer group.
* Data written to a topic can be read by multiple consumers,so make sure each our downstream app have thier own consumer group ,so that they can read the entirety of data from the topic.
* If we add a new consumer to a consumer group then this new consumer will get some partition allocated and the old consumer which used to get data from this partition will NOT get it anymore.Same happens when a consumer goes down in a consumer group or when admin adds a partition to a topic\(NOT a good thing ,due to chnages in partitioining of keys\).
* This process is called "rebalancing",during this time whole consumer group becomes unavailable ,so we need to be careful  about these rebalancing.
* Consumers keep sending a regular heart beats to a consumer group co-ordinator broker.This may be different for different consumer group.It sends heart beat when it polls to get records and commits when it had read the message.
* If a consumer does NOT send a heartbeat long enough then the group co-ordinator will decide that consumer is dead and retrigger a rebalancing.We can control the frequency of heartbeats and duration after which group co-ordinator decides that consumer is dead.
* Consumer which wants to join a group sends a JoinGroup request to the GC\(Group Co-ordinator\) broker,the first consumer to join the group will be the leader and GC will send all ther consumers which info who want to join the group,then the leader consumer using a partitioing policy allocates partitions to the consumers and sends this info to the GC.Then GC sends only the required info a individual consumer saying from whicj partition they need to read from.ONly the GC and Consumer leader has the whole assignment list.This process will be done everytime when there is a rebalancing.&lt;Q**uestion** : So can the Consumer leader change everytime there is a relanacing??&gt;
* You will need a KafkaConumer object with properties ,mandatory properties key.deserializer,value.deserializer and bootstrap.servers.good to always give group.id ,this will determine the consumer group name.
* A consumer can subsrcibe to multiple topics and we have a feature to use regular expression and whenever a new topic gets added ,then rebalancing will happen. The subscribe method has 3 overloaded types.
* Polling is the main method which controls co-ordinations heartbeats,rebalancing,fetching data.
* poll method takes a Long which determines for how long\(in ms\) consumer will block before fetching data .This will return a ConsumerRecords\[K,V\] object which is a List/Iterable where each element corresponds to a ConsumerRecord from each partition it has read/fetched data for a particular topic.
* Always close the consumer ,this will amke sure if the consumer dies ,GC comes to know about it asap.
* The poll loop does a lot more than just get data. The first time you call poll\(\) with a new consumer, it is responsible for finding the GroupCoordinator, joining the consumer group, and receiving a partition assignment. If a rebalance is triggered, it will be handled inside the poll loop as well. And of course the heartbeats that keep consumers alive are sent from within the poll loop. For this reason, we try to make sure that whatever processing we do between iterations is fast and efficient.
* When a consumer does a poll for the first time is when it joins a group.
* Rule is to have one thread or one application per consumer.Do NOT have multiple threads having consumer or multiple consumers in one thread. [https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/](https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/)

## Imp Properties

* Most of them are good enough to be kep in default.
* fetch.min.bytes =&gt; This will make the consumer to wait until it can fetch atleast this many bytes.This will reduce the two and forth movement of every small data.Look at cpu consumption of consumer and see if its very hgh when when very less data in topics then you can increase this value.
* fetch.max.wait.ms =&gt; default is 500 ms,max wait time before fetching.If you set fetch.max.wait.ms to 100 ms and fetch.min.bytes to 1 MB, Kafka will recieve a fetch request from the consumer and will respond with data either when it has 1 MB of data to return or after 100 ms, whichever happens first.
* max.partition.fetch.bytes =&gt; Default is 1MB.This is the maximum amout of bytes a consumer can read per partition.Remember when you a do a poll,it a returns a Iterable ConsumerRecords which has ConsumerRecord object per partition read by that Consumer.No assume we have 20 partitions and we have 5 consumers,then each consumer can read maximum of 4MB,this may be very less and we may have a possibility of consumers dying and also we need to make sure this is greater then max.message.bytes which detemines max size of a message in broker.Also we need to be careful not to make this too high,beacuse then we will need a lot of time to process it and delaying the poll.
* Now consumer will keeps polling and also internally keeps sending heartbeats based on heartbeat.interval.ms.Now if it does send a heartbeat or poll for a total of session.timeout.ms\(default is 3 seconds\) then GC will think this consumer is dead and it will rebalance.Also if a consumer is sending heartbeat but its not polling for a total of max.poll.interval.ms ,then also GC will consider consumer dead and rebalance.This is to avoid "livelock" situation
* max.poll.records =&gt; per poll how max many records it can take,&lt;Question max.partition.fetch.bytes and max.poll.records which one takes preference??&gt;
* auto.offset.reset=&gt;This property controls the behavior of the consumer when it starts reading a partition for which it doesn’t have a committed offset or if the committed offset it has is invalid \(usually because the consumer was down for so long that the record with that offset was already aged out of the broker\). The default is “latest,” which means that lacking a valid offset, the consumer will start reading from the newest records \(records that were written after the consumer started running\). The alternative is “earliest,” which means that lacking a valid offset, the consumer will read all the data in the partition, starting from the very beginning.
* enable.auto.commit=&gt;This parameter controls whether the consumer will commit offsets automatically, and defaults to true. Set it to false if you prefer to control when offsets are committed, which is necessary to minimize duplicates and avoid missing data. If you set enable.auto.commit to true, then you might also want to control how frequently offsets will be committed using auto.commit.interval.ms.



